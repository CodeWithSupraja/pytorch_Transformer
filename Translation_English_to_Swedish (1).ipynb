{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzin-YV7R9WQ"
   },
   "source": [
    "**Translation - English to Swedish using Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oOAc_fP5YPU"
   },
   "source": [
    "**Installing necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG317rIhBGze"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install tqdm\n",
    "!pip install datasets\n",
    "!pip install pathlib\n",
    "!pip install tokenizers\n",
    "!pip install wandb\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dgq3_k6uak0J"
   },
   "source": [
    "**Transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0YJq1ordcPO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "\n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY71kytabMc0"
   },
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeG4B_esdkoA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pd3B5ZdanX9"
   },
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We9_q00edol4"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": \"opus_books\",\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"sv\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfrdUtKTbcDj"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a4003d2e4f974005bac38403b1cf5e6e",
      "bd1ea3b8f4f64d47a3ad58b7adef0d9d",
      "40e327eacfc041d3b4fb13e281435d9f",
      "f8849cf6e3c6428586145bb99c43a84d",
      "5d73651d7f9143a4b27c912e83fcdd6f",
      "93f90482f48b45bfa53f385dc982a4cf",
      "c1bd690c14fa4f0a803cbca30c6baa35",
      "1162c27d68b44564a98351bfa392b631",
      "c14f92bf1ed24daeb7423e2e6b1511a6",
      "518274d987bf41638b5c28ad56a397e3",
      "c6352c7023c443a8ba028420a193f21d",
      "0a12e4e858d14f08b1aa6910846d3292",
      "91a00f7731be480581f447e285e48652",
      "88294f5b43f34a108a8d4028b60552c9",
      "b9b4b20a9a3740aaa516189611145b48",
      "cde1740e444a47efbbb6ab6a655cbfaa",
      "04ec02376561405c96d950a3721b51cd",
      "7757dbb076e246678feb74b5097a5589",
      "211263dd604b46dfb47e6a1f6cc1e4a4",
      "2e5d46eb873a46bf9dee3ec6c9723c53",
      "f23326131ace40e183a18c1c2e11b4fc",
      "7cb16c757cac4b4fbf42cb2419e6928b",
      "a2537c26fbe941be82258f112003b3d6",
      "336b33ffb2844553a26d190a95374698",
      "cc62847a4d894621a6c28b4d850bd584",
      "d53e45705b6449d9b54707cc8b11897c",
      "6135a1bb2c5a43268dc27cb66ffc61bb",
      "2b291bde8dc74ac09dc8d0fa6eacf280",
      "2dc2b334bbe540228f7c133a910d9a82",
      "d1742438668d4f359896ab1ed23978b1",
      "0c21c3bd6a1949699ed0ea713bd8864e",
      "a6290e8340b04d808d898656c12d8fbf",
      "5853eb714a9b40d494c0c21186e8255a"
     ]
    },
    "id": "3eoetAZydrgB",
    "outputId": "94a6d99c-8313-4481-aebc-ab211d53d7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4003d2e4f974005bac38403b1cf5e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a12e4e858d14f08b1aa6910846d3292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/516k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2537c26fbe941be82258f112003b3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 280\n",
      "Max length of target sentence: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 349/349 [02:01<00:00,  2.87it/s, loss=5.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: NERVOUS OLD LADY NEAR THE FIRE BEGINS TO CRY, AND HAS TO BE LED OUT.]\n",
      "    TARGET: En nervöst lagd äldre dam vid eldstaden börjar gråta och måste föras ut därifrån.]\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Rich old couples, with no one to leave their money to, die childless.\n",
      "    TARGET: Rika gamlingar, som saknar arvingar, dör barnlösa.\n",
      " PREDICTED: Vi , att , att , att , att .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 349/349 [02:05<00:00,  2.79it/s, loss=3.870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We began to understand the sufferings of the Babes in the Wood.\n",
      "    TARGET: Vi började nu begripa the Babes in the Wood lidanden.\n",
      " PREDICTED: Det var .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And we walked miles upon miles out Birmingham way; but it was no use, the country was steeped in oil.\n",
      "    TARGET: Och vi vandrade flera miles längs vägen mot Birmingham; men det var fåfängt, landsbygden var indränkt i fotogen.\n",
      " PREDICTED: Vi sade , att vi var vi var , att vi var det var det var det var det var det var vi var vi var vi var .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=5.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"They'd hardly have taken the pie too,\" said George.\n",
      "    TARGET: ”De skulle väl knappast ha tagit pajen också, i så fall”, sade George.\n",
      " PREDICTED: ” , det var det var det .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We got to chatting about our rowing experiences this morning, and to recounting stories of our first efforts in the art of oarsmanship.\n",
      "    TARGET: Vi kom att prata om våra tidiga roddarerfarenheter denna morgon och återupplivade historier om våra första försök i roddens ädla konst.\n",
      " PREDICTED: Vi hade oss i och vi hade .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=4.821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We played MORCEAUX from the old German masters.\n",
      "    TARGET: Vi lyssnade till Morceaux av de gamla tyska mästarna.\n",
      " PREDICTED: Vi .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: By clinging like grim death to the gunwale, we just managed to keep inside the boat, but it was exhausting work.\n",
      "    TARGET: Genom att hänga oss fast för glatta livet i fribordet, lyckades vi nätt och jämt stanna kvar ombord, men det var hårt arbete.\n",
      " PREDICTED: Och vi skulle vi ha , men vi skulle vi ha ha , men vi skulle vi ha , men vi ha .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=4.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The first thing was that they thought the boat was not clean.\n",
      "    TARGET: För det första ansåg de, att båten inte var ren.\n",
      " PREDICTED: var inte inte inte inte , att det var inte inte .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And, strange as it may appear, those clumps on the head often cured me - for the time being.\n",
      "    TARGET: Och hur underligt det än kan förefalla, sa bo-tade dessa örfilar mig — för stunden.\n",
      " PREDICTED: Och så , så , som , som jag på den .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=5.009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Oh, all right, I'll tell `em.\n",
      "    TARGET: ”Å, jasså! Det skall jag säga till dem.\n",
      " PREDICTED: ” Å , jag är jag att jag er .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We tried to get away from it at Marlow.\n",
      "    TARGET: Vi försökte undkomma den i Marlow.\n",
      " PREDICTED: Vi tog på den i .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=4.847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: When Montmorency meets a cat, the whole street knows about it; and there is enough bad language wasted in ten seconds to last an ordinarily respectable man all his life, with care.\n",
      "    TARGET: Då Montmorency träffar på en katt, får hela gatan reda på det; och han använder lika mycket fula ord under tio sekunder som skulle räcka åt en någorlunda anständig karl under dennes livstid, med råge.\n",
      " PREDICTED: Då en av dem , och det är mycket ; och det är att sig i att sig i .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: By-and-by a small boat came in sight, towed through the water at a tremendous pace by a powerful barge horse, on which sat a very small boy.\n",
      "    TARGET: Då och då kom en liten båt inom synhåll, dragen genom vattnet i oerhörd hastighet av en kraftfull arbetshäst, på vilken en mycket liten pojke satt.\n",
      " PREDICTED: och en av en av en , i en av en , i en .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=3.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The hired up-river boat very soon puts a stop to any nonsense of that sort on the part of its occupants.\n",
      "    TARGET: Hyrbåtarna uppströms befriar snart sina hyresgästers sinnen från alla dylika dumheter.\n",
      " PREDICTED: Det verkade vara ett ställe för mycket ställe att få sig på en av av av .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I motioned him away with silent dignity, but he still advanced, screeching out the while:\n",
      "    TARGET: Jag försökte avfärda honom med stilla värdighet, men han fortsatte mot mig, under det att han ropade:\n",
      " PREDICTED: Jag honom om honom , men han , men han tog med oss :\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=4.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It is, I suppose, Boulter's not even excepted, the busiest lock on the river.\n",
      "    TARGET: Jag tror att den är, Boulter’s inte undantagen, den mest livligt trafikerade slussen längs hela floden.\n",
      " PREDICTED: Det är , som jag inte är något på floden , på floden .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then the second man climbs out of the boat and comes to help him, and they get in each other's way, and hinder one another.\n",
      "    TARGET: Så klättrar den andre mannen ur båten och kommer honom till hjälp och de går i vägen för varandra och hindrar varandra.\n",
      " PREDICTED: Då man den andra och sig i båten , och de tog sig för att få på och de kommer att få sig på att göra .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=4.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: For drink, we took some wonderful sticky concoction of Harris's, which you mixed with water and called lemonade, plenty of tea, and a bottle of whisky, in case, as George said, we got upset.\n",
      "    TARGET: Till måltidsdryck valde vi något underbart, klistrigt hopkok som Harris ägde, vilken man blandade med vatten och kallade lemonad, gott om te och en flaska whisky, för den händelse vi skulle kantra.\n",
      " PREDICTED: För att vi en , Harris , Harris och med ett , sade att det var ett , att George sade , att George , , sade att George .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And so, with sentinel in each dark street, and twinkling watch-fires on each height around, the night has worn away, and over this fair valley of old Thame has broken the morning of the great day that is to close so big with the fate of ages yet unborn.\n",
      "    TARGET: Och så, med vaktposter vid varje mörk gata, och sprakande vakteldar på varje höjd runt omkring, har natten passerat och över denna vackra dalgång vid den gamla Themsen, har morgonen denna stora dag brutit fram, den dag som skall komma att ändas så storartat och påverka historiens lopp för all framtid.\n",
      " PREDICTED: Och så , med sin och upp och upp i , som har sin och den som har sig i den som är , som är det mest av dess .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=2.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: HARRIS (continuing):\n",
      "    TARGET: Harris: [Fortsätter]\n",
      " PREDICTED: Harris : [ ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You cannot give me too much work; to accumulate work has almost become a passion with me: my study is so full of it now, that there is hardly an inch of room for any more.\n",
      "    TARGET: Man kan inte ge mig för många arbetsuppgifter; att samla på mig arbete har sånär blivit min passion: Mitt arbetsrum är nu så fullt av det, att det knappt får plats en gnutta till.\n",
      " PREDICTED: Man kan inte mer än att mig ; så , som är så med en , som är det mer än vad som är jag kan att mig om .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=3.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Nobody seemed quite sure what it was exactly, but they all agreed that it sounded Scotch.\n",
      "    TARGET: Ingen verkade helt säker på vad det faktiskt var han spelade, men alla höll med om, att det lät som någonting skotskt.\n",
      " PREDICTED: Det verkade som om den första , men de var helt enkelt , men det gjorde det i grytan gjorde jag .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: John Edward says, \"Oh!\" he hadn't noticed it; and Emily says that papa does not like the gas lit in the afternoon.\n",
      "    TARGET: John Edward säger ”Åh”, det har han inte lagt märke till; och Emily säger att Pappa inte tycker om att man tänder gasen om eftermiddagarna.\n",
      " PREDICTED: Har de säger , ” han !” säger han , att det inte kan få sig ; och , som om någon sig i hela världen med hela världen .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 349/349 [02:05<00:00,  2.79it/s, loss=4.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Perhaps, from the casement, standing hand-in-hand, they were watching the calm moonlight on the river, while from the distant halls the boisterous revelry floated in broken bursts of faint-heard din and tumult.\n",
      "    TARGET: Kanske stående hand i hand i de franska fönstren, betraktade de det stilla månskenet över floden, under det att de från de avlägsna festsalarna hörde det larmande bullret komma flytande i brutna skurar av otydliga vrål och tumult.\n",
      " PREDICTED: , från de av de två små , där de två män vid floden , där de två män , under det att de i och som stod .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Men came with poles and ropes, and tried to separate the dogs, and the police were sent for.\n",
      "    TARGET: Karlar kom dit med pålar och rep, för att försöka sära på hundarna och man skickade efter polisen.\n",
      " PREDICTED: I med sin hustru och försökte ta sig i ordning för att sina ben och med oss .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=2.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We could see ourselves at supper there, pecking away at cold meat, and passing each other chunks of bread; we could hear the cheery clatter of our knives, the laughing voices, filling all the space, and overflowing through the opening out into the night.\n",
      "    TARGET: Vi kunde se oss äta kvällsmat där, huggande in på kallskuret och räckande varandra brödbitar; vi kunde höra våra knivars glada klapprande, våra skrattande röster som fyllde hela kapellet och strömmade ut i natten.\n",
      " PREDICTED: Vi kunde se oss på vi åter i våra , samt att vi skulle komma med ; och vi kunde våra kläder som av våra kläder som vi kunde av våra kläder och av våra kläder .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But all their heads were, by this time, in such a confused whirl that they were incapable of grasping anything, and so the man told them to stop where they were, and he would come to them.\n",
      "    TARGET: Men alla var vid det laget så snurriga i huvudet, att de var oförmögna att förstå någonting alls, så karlen sade åt dem att stå kvar där de befann sig, så skulle han komma in och hämta ut dem.\n",
      " PREDICTED: Men de var på detta sätt , då de kom att få sig att de var de som satt på dem , och de ville gå till och han skulle göra dem och lägga dem i .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=3.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Why, we can't steer, if you keep stopping.\n",
      "    TARGET: ”Jo, för om ni stannar till, så kan vi inte styra.\n",
      " PREDICTED: ” Men , vi kan inte se något annat , om man ville .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then our porter said he thought that must be it on the high-level platform; said he thought he knew the train.\n",
      "    TARGET: Så sade vår bärare att det måste finnas vid den övre plattformen; han sade sig känna till det tåget.\n",
      " PREDICTED: Så , sade att det trodde att det måste vara den saken ; den saken så han sade att han trodde att han trodde att han inte hade .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=2.063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I had been told to stand where I was, and wait till the canvas came to me, and Montmorency and I stood there and waited, both as good as gold.\n",
      "    TARGET: De hade sagt åt mig att stå där jag stod och att vänta tills segelduken kom till mig och Montmorency och jag själv stod där och väntade, trogna som guld.\n",
      " PREDICTED: Jag hade varit där där där där , var jag lade in och lade mig för mig och lade mig som jag lade i strömfåran och väntade .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: George anathematized Mrs. G. for a lazy old woman, and thought it was very strange that people could not get up at a decent, respectable time, unlocked and unbolted the door, and ran out.\n",
      "    TARGET: George öste sitt anatema över Mrs. G. och kallade henne en lat kärring, samt ansåg det vara mycket egendomligt att folk inte kunde stiga upp vid en kristlig tidpunkt som denna, låste upp och öppnade dörren och sprang iväg.\n",
      " PREDICTED: George . — George och ett över denna dag , att det inte var mycket svårt att hålla över dem , utan att sig .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=2.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: To those who do contemplate making Oxford their starting-place, I would say, take your own boat - unless, of course, you can take someone else's without any possible danger of being found out.\n",
      "    TARGET: Till dem som överväger att låta Oxford utgöra startplats, vill jag säga: Ta med er egen båt — såvida ni inte, naturligtvis, kan ta någon annans båt utan uppenbar risk att bli ertappade.\n",
      " PREDICTED: För att de hos deras båt , jag kan få det att säga mig , att man kan göra något emot att ni inte har något emot en enda arbete .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Oh, no, it's simple enough.\n",
      "    TARGET: ”Nej-nej, det är mycket enkelt.\n",
      " PREDICTED: ” Å , ingen fara ”, mumlade mycket .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=2.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They did scrape it out at last, and put it down on a chair, and Harris sat on it, and it stuck to him, and they went looking for it all over the room.\n",
      "    TARGET: De skrapade fram det till sist och lade smöret på en stol och Harris satte sig på det och det fastnade på honom och de gick runt och letade efter det i hela rummet.\n",
      " PREDICTED: Det gjorde det dock att börja med , satte den på sig och Harris satte sig och satte sig på den , satte de upp sin , för att därefter gå till och de höll upp den .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There are a certain number of riverside roughs who make quite an income, during the summer, by slouching about the banks and blackmailing weak-minded noodles in this way.\n",
      "    TARGET: Det finns ett antal skojare som tjänar en rätt duktig hacka under sommarhalvåret, genom att driva runt längs kanalbankarna och utpressa ödmjuka stackare som kommer i deras väg.\n",
      " PREDICTED: Det finns en enda dag som man säger att spela på att ett tillfälle , vilket finns en gammal och det allra mest i det att hålla deras sida .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=2.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And one would open the door and mount the steps, and stagger back into the arms of the man behind him; and they would all come and have a sniff, and then droop off and squeeze into other carriages, or pay the difference and go first.\n",
      "    TARGET: Och de öppnade dörren och klev uppför trappstegen och ryggade tillbaka i famnen på den som stod bakom dem; och så drog de ett andetag, för att därpå slinka undan och tränga in sig i andra vagnar, eller betala mellanskillnaden för att få åka i första klass.\n",
      " PREDICTED: Och en annan man kom fram och , i vattnet och upp alla de stora mörka ; och de brukade sig alla , till ett ljud och alla från dem och alla andra sidan och alla .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There is too much odour about cheese.\n",
      "    TARGET: Ost luktar helt enkelt för mycket.\n",
      " PREDICTED: Det är alltför för egen del .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=2.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I heard a man, going up a mountain in Switzerland, once say he would give worlds for a glass of beer, and, when he came to a little shanty where they kept it, he kicked up a most fearful row because they charged him five francs for a bottle of Bass.\n",
      "    TARGET: Jag hörde en man som var på väg uppför ett berg i Schweiz säga, att han skulle ge bort hela världen för ett glas öl och då vi kom fram till ett litet skjul är de förvarade ölen, skrek han som besatt, då han fick reda på att de tog fem francs för en flaska Bass.\n",
      " PREDICTED: Jag kände en gång en annan man vid , vid han säger han en kvarts mile från och då han kom fram till en stund , där han såg på med en annan båt , som skulle komma att försöka få reda på för honom att kunna komma att betala en annan annan båt .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We took up the hoops, and began to drop them into the sockets placed for them.\n",
      "    TARGET: Vi plockade fram bågarna och började sätta ner dem i hålen som var avsedda för dem.\n",
      " PREDICTED: Vi tog fram och tog dem med dem i samma säng , för att stanna till dem .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 20: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=1.925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was awful gloomy before.\"\n",
      "    TARGET: Det var mycket dystert innan.”\n",
      " PREDICTED: Det var verkligen en gång .”\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There is a tomb in Shepperton churchyard, however, with a poem on it, and I was nervous lest Harris should want to get out and fool round it.\n",
      "    TARGET: Det finns dock en grav på kyrkogården i Shepperton, som har en dikt inristad på stenen och jag var orolig att Harris skulle vilja gå i land och larva runt kring den.\n",
      " PREDICTED: Det finns en gott och i ordning med en stund , vilket var det att Harris och jag skulle ta fram sin säng .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 21: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=1.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I had looped it round slowly and cautiously, and tied it up in the middle, and folded it in two, and laid it down gently at the bottom of the boat.\n",
      "    TARGET: Jag hade vindat upp den sakta och omsorgsfullt, och knutit ihop den mittpå och vikit den dubbel, samt lagt ner den försiktigt på båtens durk.\n",
      " PREDICTED: Jag hade den där gott om den , och satt upp i stolen , och så försöker den ut det ut den där satt vi båten tillbaka .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I began to think it must be all a dream, and that I was really asleep in bed, and should wake up in a minute, and be told it was past ten.\n",
      "    TARGET: Jag började tro att det hela var en dröm, att jag låg och sov i min säng, att jag skulle komma att vakna upp om någon minut, och att man skulle säga mig, att klockan var över tio.\n",
      " PREDICTED: Jag började tro att det måste vara något bra och jag tyckte att om att i synnerhet en enda säng , när jag ville ha rört sig och sade :\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 22: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=1.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was a noble sight to see them suffering thus in silence, but it unnerved me altogether.\n",
      "    TARGET: Det var en upplyftande syn att se dem lida så under tystnad, men det gjorde mig inte mindre nervös.\n",
      " PREDICTED: Det var en stund som såg mig komma att komma ihåg vilken det , men det gick mig .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Far down the road a little cloud of dust has risen, and draws nearer and grows larger, and the pattering of many hoofs grows louder, and in and out between the scattered groups of drawn-up men, there pushes on its way a brilliant cavalcade of gay-dressed lords and knights.\n",
      "    TARGET: Långt nere längs vägen har ett litet dammoln bildats, vilket nu närmar sig och växer i storlek, och klapprandet från många hovar växer i styrka och in och ut mellan de utspridda grupperna av samlade härmän, tränger det fram en lysande kavalkad av färgglada herremän och riddare.\n",
      " PREDICTED: ner längs kanalen , ett litet ställe med av de två tillfällen som inte sett från dess och av , vars tak med dess och bland dem , vars män som stod där och andra sidan och av andra sidan och dagar .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 23: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=1.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Harris and I appeared to be struck by it at the same instant.\n",
      "    TARGET: Harris och jag verkade slås av den i ett och samma ögonblick.\n",
      " PREDICTED: Harris och jag började tro att vid detta var ett ögonblick .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then our porter said he thought that must be it on the high-level platform; said he thought he knew the train.\n",
      "    TARGET: Så sade vår bärare att det måste finnas vid den övre plattformen; han sade sig känna till det tåget.\n",
      " PREDICTED: Då så , sade han , att den trodde att han måste vara framme ; den sortens namn sade han , även han trodde att helt enkelt skulle bli .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 24: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=2.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We began to understand the sufferings of the Babes in the Wood.\n",
      "    TARGET: Vi började nu begripa the Babes in the Wood lidanden.\n",
      " PREDICTED: Vi började sova , vilka i .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Harris and I would go down in the morning, and take the boat up to Chertsey, and George, who would not be able to get away from the City till the afternoon (George goes to sleep at a bank from ten to four each day, except Saturdays, when they wake him up and put him outside at two), would meet us there.\n",
      "    TARGET: Harris och jag skulle resa ner på morgonen och ta båten upp till Chertsey och George, som inte skulle kunna komma iväg från City förrän om eftermiddagen (George sover på en bank mellan tio och fyra varje dag, utom lördagar, då de väcker honom och kastar ut honom vid två), skulle möta oss där.\n",
      " PREDICTED: Harris och jag brukade sätta oss i båten och få lite , för att George inte tala om nätterna , utan att som George ville ta oss på kanalbanken , för att försöka föra en stund klockan fyra miles från två fot två fot från två fot från två gånger ta sig klockan två gånger om nätterna .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 25: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=1.690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The hired up-river boat very soon puts a stop to any nonsense of that sort on the part of its occupants.\n",
      "    TARGET: Hyrbåtarna uppströms befriar snart sina hyresgästers sinnen från alla dylika dumheter.\n",
      " PREDICTED: Floden är en liten från floden , för att tala om att tala med den saken , som man såg på marken , av att man skall fa tag .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was not a beautiful face; it was too prematurely aged-looking, too thin and drawn, to be that; but it was a gentle, lovable face, in spite of its stamp of pinch and poverty, and upon it was that look of restful peace that comes to the faces of the sick sometimes when at last the pain has left them.\n",
      "    TARGET: Det var inget vackert ansikte; det var alltför präglat av sorger och svårigheter, alltför tunt och fårat för att vara det; men det var ett vänligt, kärleksfullt ansikte, trots att det märkts av fattigdom och bekymmer och i det fanns ett uttryck av fridfull ro, som ibland visar sig i plågades anleten, då de till sist funnit att smärtan lämnat dem.\n",
      " PREDICTED: Det var inte en stor ; det var alltför , man kunde åstadkomma vara en stor och att slåss med en ; men det var som stod i vägen för och att den och att bli de i , då Marlow stod uppe på , då de på dem , , , på dem i sig i vägen .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 26: 100%|██████████| 349/349 [02:05<00:00,  2.79it/s, loss=1.788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They finally rested upon a dusty old glass-case, fixed very high up above the chimney-piece, and containing a trout.\n",
      "    TARGET: Till sist kom de att vila på ett dammigt vitrinskåp, uppsatt mycket högt ovanför rökgången, innehållande en forell.\n",
      " PREDICTED: En del av en ångslup , endast en liten av som upp längs floden och , med ett par åror som svar .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I expect that machine must have been referring to the following spring.\n",
      "    TARGET: Jag antar att maskinen måste ha avsett den följande våren.\n",
      " PREDICTED: Jag tror att det måste ha som till .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 27: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=1.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A dense crowd watched the entertainment from Kew Bridge with much interest, and everybody shouted out to them different directions.\n",
      "    TARGET: En stor folkskara betraktade nöjet från Kew Bridge med stort intresse och alla skrek olika goda råd åt dem.\n",
      " PREDICTED: En från grupp till Walton och med en stilla så skedde och de brukade hålla dem .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: After beefsteak and porter, it says, \"Sleep!\"\n",
      "    TARGET: Efter biffstek och porter säger den: ”Sov!”\n",
      " PREDICTED: Efter varma och säger den säger den : ” Var !”\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 28: 100%|██████████| 349/349 [02:04<00:00,  2.79it/s, loss=1.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I tried to do so once.\n",
      "    TARGET: Det försökte jag en gång.\n",
      " PREDICTED: Jag försökte att säga att en gång gång .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I woke Harris, and told him.\n",
      "    TARGET: Jag väckte Harris och berättade det för honom.\n",
      " PREDICTED: Jag blev vild och Harris , och sade honom .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 29: 100%|██████████| 349/349 [02:04<00:00,  2.80it/s, loss=1.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Why, our boat's gone off!\" they replied in an indignant tone. \"We just got out to disentangle the tow-line, and when we looked round, it was gone!\"\n",
      "    TARGET: ”Jo, vår båt har åkt sin väg!” svarade de, indignerat. ”Vi klev ur för att trassla upp draglinan och då vi såg os om, var den försvunnen!”\n",
      " PREDICTED: ” Men , i vår båt !” har de ställt till med ett par gånger om , ” vi tar in vid årorna och pekade den på väg mot tiden , då vi var , var det endast !”\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Nobody spoke.\n",
      "    TARGET: Ingen sade något.\n",
      " PREDICTED: Ingen talade .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#from model import build_transformer\n",
    "#from dataset import BilingualDataset, causal_mask\n",
    "#from config import get_config, get_weights_file_path\n",
    "\n",
    "#import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "\n",
    "\n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate\n",
    "    metric = torchmetrics.CharErrorRate()\n",
    "    cer = metric(predicted, expected)\n",
    "    wandb.log({'validation/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric(predicted, expected)\n",
    "    wandb.log({'validation/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric(predicted, expected)\n",
    "    wandb.log({'validation/BLEU': bleu, 'global_step': global_step})\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "def get_ds(config):\n",
    "\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model\n",
    "\n",
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    # Make sure the weights folder exists\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "        del state\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    # define our custom x axis metric\n",
    "    wandb.define_metric(\"global_step\")\n",
    "    # define which metrics will be plotted against it\n",
    "    wandb.define_metric(\"validation/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    config['num_epochs'] = 30\n",
    "    config['preload'] = None\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"pytorch-transformer\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6S056c6Vkr8"
   },
   "source": [
    "**Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "EzjBHHg28odI",
    "outputId": "df0bdc53-2f40-4135-fd4a-cdb2b88113bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "    SOURCE: -f\n",
      " PREDICTED:        "
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#from config import get_config, latest_weights_file_path\n",
    "#from model import build_transformer\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_dataset\n",
    "#from dataset import BilingualDataset\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "def translate(sentence: str):\n",
    "    # Define the device, tokenizers, and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    config = get_config()\n",
    "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
    "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
    "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "    # if the sentence is a number use it as an index to the test set\n",
    "    label = \"\"\n",
    "    if type(sentence) == int or sentence.isdigit():\n",
    "        id = int(sentence)\n",
    "        ds = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='all')\n",
    "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "        sentence = ds[id]['src_text']\n",
    "        label = ds[id][\"tgt_text\"]\n",
    "    seq_len = config['seq_len']\n",
    "\n",
    "    # translate the sentence\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Precompute the encoder output and reuse it for every generation step\n",
    "        source = tokenizer_src.encode(sentence)\n",
    "        source = torch.cat([\n",
    "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
    "            torch.tensor(source.ids, dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "        ], dim=0).to(device)\n",
    "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "        # Initialize the decoder input with the sos token\n",
    "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "\n",
    "        # Print the source sentence and target start prompt\n",
    "        if label != \"\": print(f\"{f'ID: ':>12}{id}\")\n",
    "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
    "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\")\n",
    "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
    "\n",
    "        # Generate the translation word by word\n",
    "        while decoder_input.size(1) < seq_len:\n",
    "            # build mask for target and calculate output\n",
    "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # project next token\n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "            # print the translated word\n",
    "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
    "\n",
    "            # break if we predict the end of sentence token\n",
    "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
    "                break\n",
    "\n",
    "    # convert ids to tokens\n",
    "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
    "\n",
    "#read sentence from argument\n",
    "translate(sys.argv[1] if len(sys.argv) > 1 else \"I am not a very good a student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSrwgDsxePUw",
    "outputId": "6561240e-540f-419a-efe7-2b8786495ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence: 280\n",
      "Max length of target sentence: 269\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And out he went, and left us alone.\n",
      "    TARGET: Och så gick han och lämnade oss ensamma.\n",
      " PREDICTED: Och så gick han och lämnade oss ensamma .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"If I am guilty,\" said the Earl, \"may this bread choke me when I eat it!\"\n",
      "    TARGET: ”Om jag är skyldig”, sade earlen, ”må då detta brödstycke kväva mig, då jag äter det!”\n",
      " PREDICTED: ” Om jag är skyldig ”, sade , ” må då detta mig , då jag äter det !”\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I never saw such a thing as potato-scraping for making a fellow in a mess.\n",
      "    TARGET: Jag har aldrig varit med om något värre än att borsta potatis i fråga om att ställa till en karl.\n",
      " PREDICTED: Jag har aldrig varit med om något värre än att borsta potatis om att ställa till en karl .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was horribly lonesome and dismal, and all the policemen he met regarded him with undisguised suspicion, and turned their lanterns on him and followed him about, and this had such an effect upon him at last that he began to feel as if he really had done something, and he got to slinking down the by-streets and hiding in dark doorways when he heard the regulation flip-flop approaching.\n",
      "    TARGET: Det var fruktansvärt ensamt och dystert och alla polismän han mötte betraktade honom med ohöljd misstänksamhet och lyste med sina lyktor på honom och följde efter honom. Detta fick till följd att han till sist kände sig skyldig och han började slinka in på bakgator och gömma sig i mörka portgångar, då han hörde lagens väktares steg närma sig.\n",
      " PREDICTED: Det var fruktansvärt och dystert och alla han mötte betraktade honom med misstänksamhet och lyste med sina på honom och följde efter honom . Detta fick till följd att han till sist kände sig skyldig och han började slinka in på och sig i mörka , då han hörde steg steg sig .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They were the most disgraceful pickles I ever tasted in a respectable boat.\n",
      "    TARGET: De serverade den mest gräsliga syltade lök jag nagonsin smakat pa en i övrigt anständig bat.\n",
      " PREDICTED: De bar den allra mest trivsamma sällskap som jag någonsin på en båt .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Yes!\n",
      "    TARGET: ”Jaha?\n",
      " PREDICTED: ” Jaha ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: George told him that his appearance, after the course, did not seem a sufficiently good advertisement for the brand; and that he would prefer it out of a pump.\n",
      "    TARGET: George sade, att mannens utseende, efter denna tidsrymd, inte föreföll honom vara tillräckligt god sägen för varans kvalitet; och att han föredrog vatten som kom ur en pump.\n",
      " PREDICTED: George sade , att mannens utseende , efter denna , inte föreföll honom vara tillräckligt god för ; och att han föredrog vatten som kom ur en .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They represent themselves as sent by the proprietor.\n",
      "    TARGET: De säger sig vara utsända av markägaren.\n",
      " PREDICTED: De genom att krossa en , för att kunna .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: (It surprises me myself, sometimes, how many of these subjects there are.)\n",
      "    TARGET: (Det förvånar ibland till och med mig själv, hur många de frågorna är.)\n",
      " PREDICTED: ( Det ibland till och med mig själv , hur många de är .)\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I hop back and dress, and crawl home, where I have to pretend I liked it.\n",
      "    TARGET: Jag hoppar tillbaka till land, krälar hem, där jag tvingas låtsas att jag njutit av det hela.\n",
      " PREDICTED: Jag hoppar tillbaka till land , hem , där jag tvingas låtsas att jag njutit av det hela .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from config import get_config, latest_weights_file_path\n",
    "#from train import get_model, get_ds, run_validation\n",
    "#from translate import translate\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjSeuaTl9C4C",
    "outputId": "18d82aa9-6753-4adb-e42f-78f9f942539a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "    SOURCE: And out he went, and left us alone.\n",
      " PREDICTED: Och så gick han och lämnade ensamma oss ensamma .  "
     ]
    }
   ],
   "source": [
    "t = translate(\"And out he went, and left us alone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I24uV6dC9VXK",
    "outputId": "5a4914b3-f6b4-4fea-d499-dff268b8a38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "        ID: 34\n",
      "    SOURCE: Then I wondered how long I had to live.\n",
      "    TARGET: Sa undrade jag över hur länge jag hade kvar att leva.\n",
      " PREDICTED: Sa undrade jag hur jag hade jag att leva att leva .  "
     ]
    }
   ],
   "source": [
    "t = translate(34)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
